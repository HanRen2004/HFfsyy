import streamlit as st
import requests
import uuid
import dashscope
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.tools import tool
from langchain_community.llms.tongyi import Tongyi
import os
from langchain.agents import initialize_agent, AgentType, Tool
from langchain_core.prompts import PromptTemplate
from openai import OpenAI
from langchain_openai import ChatOpenAI
from flask import Flask, request, jsonify
from streamlit.components.v1 import html
import base64
import threading
import time

# é¡µé¢é…ç½®
st.set_page_config(page_title="æ”¾æ¾éŸ³ä¹", page_icon="ğŸµ ")

# è‡ªå®šä¹‰ CSS æ ·å¼
custom_css = """
<style>
    h1 {
        font-size: 2.5rem;
        color: #333;
        text-align: center;
    }

    h2 {
        font-size: 1.5rem;
        color: #666;
        text-align: center;
        margin-bottom: 20px;
    }

    .result-block {
        border: 1px solid #ddd;
        padding: 15px;
        margin-bottom: 20px;
        border-radius: 8px;
        background-color: #f9f9f9;
    }

    .result-block h3 {
        font-size: 1.2rem;
        color: #4a4a4a;
        margin-bottom: 10px;
    }

    .result-block p {
        font-size: 1rem;
        color: #555;
        margin-bottom: 5px;
    }

    .result-block a {
        display: inline-block;
        margin-top: 10px;
        padding: 8px 12px;
        background-color: #007BFF;
        color: white;
        text-decoration: none;
        border-radius: 5px;
    }
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)

# é¡µé¢æ ‡é¢˜å’Œå°æ ‡é¢˜
st.title("æ”¾æ¾éŸ³ä¹")
st.markdown('<h2 style="font-family: Arial;">æ”¾æ¾å¿ƒæƒ…ï¼Œç¼“è§£å‹åŠ›</h2>', unsafe_allow_html=True)

# è®¾ç½® API å¯†é’¥
os.environ["DASHSCOPE_API_KEY"] = os.getenv("DASHSCOPE_API_KEY", "sk-38a6f574d6c6483eae5c32998a16822a")
os.environ["DASHSCOPE_API_BASE"] = os.getenv("DASHSCOPE_API_BASE", "https://dashscope.aliyuncs.com/compatible-mode/v1")



# åˆ›å»ºç½‘ç»œæœç´¢å·¥å…·
@tool
def bocha_websearch_tool(query: str, count: int = 20) -> str:
    """
    ä½¿ç”¨Bocha Web Search API ç½‘é¡µæœç´¢
    """
    url = 'https://api.bochaai.com/v1/web-search'
    headers = {
        'Authorization': f'Bearer sk-6012a020f72d4c26ae5ad415300c94f9',
        'Content-Type': 'application/json'
    }
    data = {
        "query": query,
        "freshness": "noLimit",
        "summary": True,
        "count": count
    }

    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        try:
            json_response = response.json()
            if json_response["code"] == 200 and json_response.get("data"):
                webpages = json_response["data"]["webPages"]["value"]
                if not webpages:
                    return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ."
                formatted_results = ""
                for idx, page in enumerate(webpages, start=1):
                    formatted_results += (
                        f"å¼•ç”¨ï¼š{idx}\n"
                        f"æ ‡é¢˜ï¼š{page['name']}\n"
                        f"URL: {page['url']}\n"
                        f"æ‘˜è¦ï¼š{page['summary']}\n"
                        f"ç½‘ç«™åç§°ï¼š{page['siteName']}\n"
                        f"ç½‘ç«™å›¾æ ‡ï¼š{page['siteIcon']}\n"
                        f"å‘å¸ƒæ—¶é—´ï¼š{page['dateLastCrawled']}\n\n"
                    )
                return formatted_results.strip()
            else:
                return f"æœç´¢å¤±è´¥ï¼ŒåŸå› ï¼š{json_response.get('message', 'æœªçŸ¥é”™è¯¯')}"
        except Exception as e:
            return f"å¤„ç†æœç´¢ç»“æœå¤±è´¥ï¼ŒåŸå› æ˜¯ï¼š{str(e)}\nåŸå§‹å“åº”ï¼š{response.text}"
    else:
        return f"æœç´¢APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}, é”™è¯¯ä¿¡æ¯ï¼š{response.text}"


memory = ConversationBufferMemory(memory_key="chat_history",return_messages=True)

llm = ChatOpenAI(
    model="qwen-max",
    temperature=0.8,
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

bocha_tool = Tool(
    name="Bocha Web Search",
    func=bocha_websearch_tool,
    description="ä½¿ç”¨Bocha Web Search APIè¿›è¡Œæœç´¢äº’è”ç½‘ç½‘é¡µï¼Œè¾“å…¥åº”ä¸ºæœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œè¾“å‡ºå°†è¿”å›æœç´¢ç»“æœçš„è¯¦ç»†ä¿¡æ¯ã€‚åŒ…æ‹¬ç½‘é¡µæ ‡é¢˜ã€ç½‘é¡µURL",
)



#æœç´¢å·¥å…·æç¤ºè¯
agent_prompt = """
å›½å†…ç›´è¿ è‡ªç„¶ç™½å™ªéŸ³ å•æ›²æ’­æ”¾
æ— ç™»é™†è¦æ±‚ ç©ºçµåŸå”±
ç½‘é¡µéŸ³ä¹ çº¯éŸ³ä¹ å•æ›²ç›´é“¾
æ— éœ€ä¸‹è½½
HTTPçŠ¶æ€ç 200 ç›´æ¥è®¿é—®
è¯»å–åœ¨bocha_toolè¿”å›ç»“æœä¸­å¯ç”¨çš„ç½‘å€é“¾æ¥ï¼Œå¹¶è¿”å›
"""


agent = initialize_agent(
    tools=[bocha_tool],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    memory=memory,
    verbose=True,
    agent_kwargs={"agent_prompt": agent_prompt, 'memory': memory}
)




#å¤§è¯­è¨€æ¨¡å‹æç¤ºè¯
prompt_template_with_search_results = """
{previous_conversation}

æœ€æ–°çš„æœç´¢ç»“æœå¦‚ä¸‹ï¼š
{search_results}

è¯·æ¨èé€‚åˆå¿ƒç†æ”¾æ¾åœºæ™¯çš„æ²»æ„ˆç³»å•æ›²èµ„æºï¼Œè¦æ±‚ï¼š
1ã€æä¾›å›½å†…å¯ç›´æ¥ç½‘é¡µæ’­æ”¾çš„ç›´é“¾ï¼ˆæ— éœ€ç™»å½•/ç¿»å¢™ï¼‰
2.æ ‡æ³¨CCBæˆæƒæˆ–å¹³å°å…è´¹è¯•å¬éŸ³ä¹
3ã€å®Œå…¨æ’é™¤ç–¾ç—…ç›¸å…³æè¿°ï¼ˆåŒ…æ‹¬éšå–»è¯æ±‡ï¼‰
4ã€è¾“å‡ºæ ¼å¼ï¼šæ­Œæ›²åç§° +åˆ›ä½œè€…+æ—¶é•¿+å›½å†…ç›´è¿å¹³å°åç§°+ç›´æ¥è®¿é—®é“¾æ¥ï¼ˆHTTPSåè®®ï¼Œå·²éªŒè¯å¯è®¿é—®ï¼Œå¹¶ä¸”åº”è¯¥åœ¨bocha_toolçš„æœç´¢è¿”å›ç»“æœï¼‰
5.æ¨èçº¯å™¨ä¹æˆ–è‡ªç„¶éŸ³æ•ˆç±»éŸ³ä¹ï¼Œæˆ–è€…å…¶ä»–ç±»å‹çš„è½»éŸ³ä¹
6.ä¼˜å…ˆä½¿ç”¨å›½å†…CDNåŠ é€Ÿèµ„æº
"""

final_prompt = PromptTemplate(
    input_variables=["previous_conversation", "search_results"],
    template=prompt_template_with_search_results
)



llm_chat = ChatOpenAI(
    model="qwen-max-latest",
    temperature=0.8,
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

chain = LLMChain(llm=llm_chat, prompt=final_prompt)





#ç”¨æˆ·æé—®ï¼ˆåŠŸèƒ½ç›¸å…³ï¼‰
user_question = "æˆ‘æƒ³å¾—åˆ°ä¸€äº›æ”¾æ¾éŸ³ä¹,è¯·ç»™æˆ‘å¯ç”¨çš„ç½‘ç»œé“¾æ¥"


response = agent.run(user_question)

# å‡†å¤‡è¾“å…¥ç»™ Final Prompt çš„æ•°æ®
inputs = {
    "previous_conversation": "\n".join([str(message) for message in memory.load_memory_variables({})["chat_history"]]),
    "search_results": response
}

final_response=chain.run(inputs)


st.write(final_response)
